{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcc63bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8a3cac",
   "metadata": {},
   "source": [
    "using PreNorm instead of AddNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7ac23d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, d_model, sublayer, dropout_rate=0.1):\n",
    "        super(PreNorm, self).__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.sublayer = sublayer\n",
    "\n",
    "    def forward(self,x):\n",
    "        normalized_output = self.norm(x)\n",
    "        sublayer_output = self.sublayer(normalized_output)\n",
    "        dropped_output = self.dropout(sublayer_output)\n",
    "        return x + dropped_output\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2275e544",
   "metadata": {},
   "source": [
    "using GLU instead of Position wise FFN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce6e3d7",
   "metadata": {},
   "source": [
    "using GELU instead of ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81ea56c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLUFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=None, dropout_rate = 0.1):\n",
    "        super(GLUFeedForward, self).__init__()\n",
    "        d_ff = d_ff or d_model\n",
    "        self.gate_proj = nn.Linear(d_model , d_ff)\n",
    "        self.value_proj = nn.Linear(d_model, d_ff)\n",
    "        self.output_proj = nn.Linear(d_ff, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate = self.gate_proj(x)\n",
    "        value = self.value_proj(x)\n",
    "\n",
    "        x = F.gelu(gate)*value\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        #project back to d_model\n",
    "        x = self.output_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9642da",
   "metadata": {},
   "source": [
    "updating Multi head attention with FAVOR+ Fast attention via positive orthogonal random features kindof linear attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6b1e271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def orthogonal_random_features(dim, num_heads, head_dim, device):\n",
    "    rand_proj = torch.randn(num_heads, dim, head_dim, device=device)\n",
    "    q, _ = torch.linalg.qr(rand_proj)\n",
    "    return q \n",
    "\n",
    "\n",
    "def elu_kernal(x):\n",
    "    return F.elu(x) + 1\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, d_model , num_heads, dropout_rate = 0.1, num_rfs = 10):\n",
    "        super(LinearAttention, self).__init__()\n",
    "        assert d_model % num_heads ==0\n",
    "\n",
    "        self.d_k =d_model // num_heads\n",
    "        self.d_model = d_model\n",
    "        self.num_rfs = num_rfs # number of random features\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        #Learnable scale Parameter for queries / keys\n",
    "        self.scale = nn.Parameter(torch.tensor(d_model ** -0.5))\n",
    "\n",
    "    def linear_attention(self, Q, K, V):\n",
    "        # Q, K, V : shape = (batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "        batch_size, num_heads, seq_len, head_dim = Q.shape\n",
    "\n",
    "        #random feature projection one per head\n",
    "        if not hasattr(self, 'rand_projs'):\n",
    "            self.rand_projs = orthogonal_random_features(\n",
    "                self.d_model, self.num_heads , self.num_rfs, Q.device\n",
    "            )\n",
    "\n",
    "        #projecting Q and K through random features\n",
    "        Q = Q.unsqueeze(-2) #(b, h, s, 1, d_k)\n",
    "        K = K.unsqueeze(-2)\n",
    "\n",
    "        Q_rand = torch.matmul(Q, self.rand_projs[:num_heads])  #(b, h, s, 1, r)\n",
    "        K_rand = torch.matmul(K, self.rand_projs[:num_heads]) #(b,h,s,1,r)\n",
    "\n",
    "        #applying kernal function\n",
    "        Q_feat = elu_kernal(Q_rand).squeeze(-2)    #(b, h, s, r)\n",
    "        K_feat = elu_kernal(K_rand).squeeze(-2)  #(b, h, s, r)\n",
    "\n",
    "        #compute KV numerator: (b,h,r,d)\n",
    "        K_feat_V = torch.einsum('bhsv, bhsd-> bhvd' , K_feat, V)\n",
    "\n",
    "        #denominator: (b, h, r)\n",
    "        z_denom = 1. / (torch.einsum('bhsr , bhs->bhr', Q_feat, torch.sum(K_feat, dim=2)) + 1e-6)\n",
    "\n",
    "        #numerator: (b, h, s, d)\n",
    "        attn_output = torch.einsum('bhsr, bhvd->bhsd', Q_feat, K_feat_V)\n",
    "\n",
    "        #normalize\n",
    "        attn_output = attn_output * z_denom.unsqueeze(-1)\n",
    "\n",
    "        return attn_output\n",
    "    \n",
    "\n",
    "    def forward(self, query_input, key_input , value_input, mask=None):\n",
    "        batch_size = query_input.size(0)\n",
    "\n",
    "        Q = self.W_q(query_input)\n",
    "        K = self.W_k(key_input)\n",
    "        V = self.W_v(value_input)\n",
    "\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
    "\n",
    "        attn_output = self.linear_attention(Q, K , V)\n",
    "\n",
    "        attn_output = attn_output.transpose(1,2).contiguous().view(batch_size , -1, self.d_model)\n",
    "        output = self.W_o(attn_output)\n",
    "\n",
    "        return output, None #no attention weights returned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000ab4ec",
   "metadata": {},
   "source": [
    "using RoPE instead of normal Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c94d5b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len=2048, base=10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.base = base\n",
    "\n",
    "        # Compute inverse frequency vector\n",
    "        inv_freq = 1. / (self.base ** (torch.arange(0, self.dim, 2).float() / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "\n",
    "        # Precompute position encodings\n",
    "        t = torch.arange(self.max_seq_len, dtype=torch.float32)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)  # Shape: (seq_len, dim)\n",
    "\n",
    "        # Apply sin/cos to create positional embeddings\n",
    "        self.register_buffer(\"cos_cached\", emb.cos()[None, :, :])\n",
    "        self.register_buffer(\"sin_cached\", emb.sin()[None, :, :])\n",
    "\n",
    "    def forward(self, x, seq_dim=1):\n",
    "        seq_len = x.shape[seq_dim]\n",
    "\n",
    "        if seq_dim == 0:\n",
    "            cos_pos = self.cos_cached[:, :seq_len, ...]\n",
    "            sin_pos = self.sin_cached[:, :seq_len, ...]\n",
    "        else:\n",
    "            cos_pos = self.cos_cached[:, :seq_len, ...]\n",
    "            sin_pos = self.sin_cached[:, :seq_len, ...]\n",
    "\n",
    "        x_rotated = rotate_half(x)\n",
    "        return (x * cos_pos) + (x_rotated * sin_pos)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
