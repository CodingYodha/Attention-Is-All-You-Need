{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "bcc63bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8a3cac",
   "metadata": {},
   "source": [
    "using PreNorm instead of AddNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f7ac23d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, d_model, sublayer, dropout_rate=0.1):\n",
    "        super(PreNorm, self).__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.sublayer = sublayer\n",
    "\n",
    "    def forward(self, x, key_input=None, value_input=None):\n",
    "        normalized_output = self.norm(x)\n",
    "        if key_input is None and value_input is None:\n",
    "            sublayer_output = self.sublayer(normalized_output)\n",
    "        else:\n",
    "            sublayer_output = self.sublayer(normalized_output, key_input=key_input, value_input=value_input)\n",
    "        dropped_output = self.dropout(sublayer_output)\n",
    "        return x + dropped_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2275e544",
   "metadata": {},
   "source": [
    "using GLU instead of Position wise FFN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce6e3d7",
   "metadata": {},
   "source": [
    "using GELU instead of ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "81ea56c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLUFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=None, dropout_rate = 0.1):\n",
    "        super(GLUFeedForward, self).__init__()\n",
    "        d_ff = d_ff or d_model\n",
    "        self.gate_proj = nn.Linear(d_model , d_ff)\n",
    "        self.value_proj = nn.Linear(d_model, d_ff)\n",
    "        self.output_proj = nn.Linear(d_ff, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate = self.gate_proj(x)\n",
    "        value = self.value_proj(x)\n",
    "\n",
    "        x = F.gelu(gate)*value\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        #project back to d_model\n",
    "        x = self.output_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9642da",
   "metadata": {},
   "source": [
    "updating Multi head attention with FAVOR+ Fast attention via positive orthogonal random features kindof linear attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8b2ceea9",
   "metadata": {},
   "outputs": [],
   "source": [
    " def linear_attention(self, Q, K, V):\n",
    "        # Q, K, V : shape = (batch_size, num_heads, seq_len, head_dim)\n",
    "        batch_size, num_heads, seq_len, head_dim = Q.shape\n",
    "\n",
    "        # Random feature projection one per head\n",
    "        if not hasattr(self, 'rand_projs'):\n",
    "            self.rand_projs = orthogonal_random_features(\n",
    "                head_dim, self.num_heads, self.num_rfs, Q.device\n",
    "            )\n",
    "\n",
    "        # Projecting Q and K through random features\n",
    "        Q_rand = torch.einsum('bhsd,hrd->bhsr', Q, self.rand_projs)  # (b, h, s, r)\n",
    "        K_rand = torch.einsum('bhsd,hrd->bhsr', K, self.rand_projs)  # (b, h, s, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a6b1e271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def orthogonal_random_features(dim, num_heads, num_rfs, device):\n",
    "    # Initialize random features for each head\n",
    "    rand_proj = torch.randn(num_heads, num_rfs, dim, device=device)\n",
    "    q, _ = torch.linalg.qr(rand_proj.transpose(-2, -1))\n",
    "    return q.transpose(-2, -1) \n",
    "\n",
    "\n",
    "def elu_kernal(x):\n",
    "    return F.elu(x) + 1\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, d_model , num_heads, dropout_rate = 0.1, num_rfs = 10):\n",
    "        super(LinearAttention, self).__init__()\n",
    "        assert d_model % num_heads ==0\n",
    "\n",
    "        self.d_k =d_model // num_heads\n",
    "        self.d_model = d_model\n",
    "        self.num_rfs = num_rfs # number of random features\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        #Learnable scale Parameter for queries / keys\n",
    "        self.scale = nn.Parameter(torch.tensor(d_model ** -0.5))\n",
    "\n",
    "    def linear_attention(self, Q, K, V):\n",
    "        # Q, K, V : shape = (batch_size, num_heads, seq_len, head_dim)\n",
    "        batch_size, num_heads, seq_len, head_dim = Q.shape\n",
    "\n",
    "        # Random feature projection one per head\n",
    "        if not hasattr(self, 'rand_projs'):\n",
    "            self.rand_projs = orthogonal_random_features(\n",
    "                head_dim, self.num_heads, self.num_rfs, Q.device\n",
    "            )\n",
    "\n",
    "        # Projecting Q and K through random features\n",
    "        Q_rand = torch.einsum('bhsd,hrd->bhsr', Q, self.rand_projs)  # (b, h, s, r)\n",
    "        K_rand = torch.einsum('bhsd,hrd->bhsr', K, self.rand_projs)  # (b, h, s, r)\n",
    "\n",
    "        Q_rand = torch.matmul(Q, self.rand_projs[:num_heads])  #(b, h, s, 1, r)\n",
    "        K_rand = torch.matmul(K, self.rand_projs[:num_heads]) #(b,h,s,1,r)\n",
    "\n",
    "        #applying kernal function\n",
    "        Q_feat = elu_kernal(Q_rand).squeeze(-2)    #(b, h, s, r)\n",
    "        K_feat = elu_kernal(K_rand).squeeze(-2)  #(b, h, s, r)\n",
    "\n",
    "        #compute KV numerator: (b,h,r,d)\n",
    "        K_feat_V = torch.einsum('bhsv, bhsd-> bhvd' , K_feat, V)\n",
    "\n",
    "        #denominator: (b, h, r)\n",
    "        z_denom = 1. / (torch.einsum('bhsr , bhs->bhr', Q_feat, torch.sum(K_feat, dim=2)) + 1e-6)\n",
    "\n",
    "        #numerator: (b, h, s, d)\n",
    "        attn_output = torch.einsum('bhsr, bhvd->bhsd', Q_feat, K_feat_V)\n",
    "\n",
    "        #normalize\n",
    "        attn_output = attn_output * z_denom.unsqueeze(-1)\n",
    "\n",
    "        return attn_output\n",
    "    \n",
    "\n",
    "    def forward(self, query_input, key_input=None , value_input=None, mask=None):\n",
    "        batch_size = query_input.size(0)\n",
    "\n",
    "        \n",
    "        if key_input is None:\n",
    "            key_input = query_input\n",
    "        if value_input is None:\n",
    "            value_input = query_input\n",
    "\n",
    "        Q = self.W_q(query_input)\n",
    "        K = self.W_k(key_input)\n",
    "        V = self.W_v(value_input)\n",
    "\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
    "\n",
    "        attn_output = self.linear_attention(Q, K , V)\n",
    "\n",
    "        attn_output = attn_output.transpose(1,2).contiguous().view(batch_size , -1, self.d_model)\n",
    "        output = self.W_o(attn_output)\n",
    "\n",
    "        return output, None #no attention weights returned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000ab4ec",
   "metadata": {},
   "source": [
    "using RoPE instead of normal Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c94d5b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len=2048, base=10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.base = base\n",
    "\n",
    "        # Create inverse frequency vector for half the dimension\n",
    "        inv_freq = 1. / (base ** (torch.arange(0, dim//2, dtype=torch.float32) / (dim//2)))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "\n",
    "    def forward(self, x, seq_dim=1):\n",
    "        batch_size, seq_len, dim = x.shape\n",
    "        \n",
    "        # Create position indices\n",
    "        pos = torch.arange(seq_len, device=x.device).type(torch.float32)\n",
    "        \n",
    "        # Compute frequencies\n",
    "        freqs = torch.einsum('i,j->ij', pos, self.inv_freq)\n",
    "        \n",
    "        # Compute sin and cos\n",
    "        emb = torch.cat((freqs, freqs), dim=-1).view(seq_len, dim//2, 2)\n",
    "        cos = emb[..., 0].view(1, seq_len, dim//2)\n",
    "        sin = emb[..., 1].view(1, seq_len, dim//2)\n",
    "        \n",
    "        # Expand to match input batch size\n",
    "        cos = cos.expand(batch_size, -1, -1)\n",
    "        sin = sin.expand(batch_size, -1, -1)\n",
    "        \n",
    "        # Split input into half for rotation\n",
    "        x1, x2 = x.chunk(2, dim=-1)\n",
    "        \n",
    "        # Apply rotation using sin and cos\n",
    "        rotated = torch.cat([\n",
    "            x1 * cos - x2 * sin,\n",
    "            x2 * cos + x1 * sin\n",
    "        ], dim=-1)\n",
    "        \n",
    "        return rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3258f8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model , num_heads, d_ff, dropout_rate=0.1, num_rfs=64):\n",
    "        super(EncoderLayer,self).__init__()\n",
    "\n",
    "        self.self_attn = LinearAttention(\n",
    "            d_model = d_model,\n",
    "            num_heads= num_heads,\n",
    "            dropout_rate= dropout_rate,\n",
    "            num_rfs=num_rfs\n",
    "        )\n",
    "\n",
    "        #preNorm wrappers \n",
    "        self.attn_norm = PreNorm(d_model , self.self_attn)\n",
    "\n",
    "        # GLU FFN\n",
    "        self.ffn = GLUFeedForward(\n",
    "            d_model=d_model,\n",
    "            d_ff=d_ff,\n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "\n",
    "        self.ffn_norm = PreNorm(d_model, self.ffn)\n",
    "\n",
    "\n",
    "        # RoPE\n",
    "        self.rope = RotaryPositionalEmbedding(d_model)\n",
    "\n",
    "\n",
    "    def forward(self, x, src_mask = None):\n",
    "        x_pos = self.rope(x) #(batch_size, seq_len, d_model)\n",
    "        x = self.attn_norm(x_pos) #self attention sublayer with prenorm includes residual connection internally\n",
    "        x = self.ffn_norm(x) #feed forward network sublayer with PreNorm\n",
    "\n",
    "\n",
    "        return x, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3f1c3212",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, dropout_rate=0.1, num_rfs=64):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(\n",
    "                d_model = d_model,\n",
    "                num_heads= num_heads,\n",
    "                d_ff = d_ff,\n",
    "                dropout_rate=dropout_rate,\n",
    "                num_rfs= num_rfs\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, src_mask = None):\n",
    "        for layer in self.layers:\n",
    "            x, _ = layer(x, src_mask) #second return value is none (no attention weights are being returned)\\\n",
    "        return x, None\n",
    "        \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d4c702cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model , d_ff , num_heads, dropout_rate= 0.1, num_rfs = 64):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.masked_self_attn = LinearAttention(\n",
    "            d_model = d_model,\n",
    "            num_heads= num_heads,\n",
    "            dropout_rate= dropout_rate,\n",
    "            num_rfs=num_rfs\n",
    "        )\n",
    "\n",
    "        self.self_attn_norm= PreNorm(d_model, self.masked_self_attn)\n",
    "\n",
    "#encoder decoder cross attention\n",
    "        self.encoder_decoder_attn = LinearAttention(\n",
    "            d_model= d_model,\n",
    "            num_heads= num_heads,\n",
    "            dropout_rate= dropout_rate,\n",
    "            num_rfs= num_rfs\n",
    "        )\n",
    "\n",
    "        self.encoder_decoder_attn_norm = PreNorm(d_model, self.encoder_decoder_attn)\n",
    "\n",
    "            # GLU FFN\n",
    "        self.ffn = GLUFeedForward(\n",
    "            d_model=d_model,\n",
    "            d_ff=d_ff,\n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "\n",
    "        self.ffn_norm = PreNorm(d_model, self.ffn)\n",
    "\n",
    "        self.rope = RotaryPositionalEmbedding(d_model)\n",
    "\n",
    "\n",
    "    def forward(self, x , encoder_output , src_mask = None, tgt_mask= None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input to decoder (batch_size, tgt_seq_len, d_model)\n",
    "            encoder_output: Output from encoder (batch_size, src_seq_len, d_model)\n",
    "            src_mask: Mask for encoder outputs (optional)\n",
    "            tgt_mask: Causal mask for decoder self-attention (optional)\n",
    "\n",
    "        Returns:\n",
    "            x: Updated decoder output\n",
    "            None, None: Placeholder for attention weights (not returned by linear attention)\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        x_pos = self.rope(x)\n",
    "        x = self.self_attn_norm(x_pos)\n",
    "        x = self.encoder_decoder_attn_norm(x, key_input=encoder_output, value_input=encoder_output)\n",
    "        x = self.ffn_norm(x)\n",
    "\n",
    "\n",
    "        return x, None , None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "54943f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model , num_heads, d_ff, dropout_rate = 0.1 , num_rfs = 64):\n",
    "        super(Decoder , self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(\n",
    "                d_model = d_model,\n",
    "                num_heads= num_heads,\n",
    "                d_ff = d_ff,\n",
    "                dropout_rate=dropout_rate,\n",
    "                num_rfs= num_rfs\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x , encoder_output , src_mask = None, tgt_mask = None):\n",
    "        for layer in self.layers:\n",
    "            x, _, _ = layer(x , encoder_output, src_mask , tgt_mask)\n",
    "        return x, None, None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a77e8638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers,\n",
    "        d_model,\n",
    "        num_heads,\n",
    "        d_ff,\n",
    "        input_vocab_size,\n",
    "        target_vocab_size,\n",
    "        dropout_rate=0.1,\n",
    "        num_rfs=64  # Number of random features for Linear Attention\n",
    "    ):\n",
    "        super(Transformer , self).__init__()\n",
    "\n",
    "        # Input Embeddings\n",
    "        self.src_embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "\n",
    "        # Encoder and Decoder Stacks (with RoPE, Linear Attention, GLU, PreNorm)\n",
    "        self.encoder = Encoder(\n",
    "            num_layers=num_layers,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            d_ff=d_ff,\n",
    "            dropout_rate=dropout_rate,\n",
    "            num_rfs=num_rfs\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            num_layers=num_layers,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            d_ff=d_ff,\n",
    "            dropout_rate=dropout_rate,\n",
    "            num_rfs=num_rfs\n",
    "        )\n",
    "\n",
    "        # Final projection layer\n",
    "        self.final_linear = nn.Linear(d_model, target_vocab_size)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Source token indices (batch_size, src_seq_len)\n",
    "            tgt: Target token indices (batch_size, tgt_seq_len)\n",
    "            src_mask: Optional mask for encoder (batch_size, 1, 1, src_seq_len)\n",
    "            tgt_mask: Optional causal mask for decoder (1, 1, tgt_seq_len, tgt_seq_len)\n",
    "\n",
    "        Returns:\n",
    "            final_output: Logits over target vocabulary\n",
    "            None: Placeholder (no attention weights returned in linear attention)\n",
    "        \"\"\"\n",
    "\n",
    "        # Input embeddings\n",
    "        src_embedded = self.dropout(self.src_embedding(src))\n",
    "        tgt_embedded = self.dropout(self.tgt_embedding(tgt))\n",
    "\n",
    "        # Encoder pass\n",
    "        encoder_output, _ = self.encoder(src_embedded, src_mask)\n",
    "\n",
    "        # Decoder pass\n",
    "        decoder_output, _, _ = self.decoder(tgt_embedded, encoder_output, src_mask, tgt_mask)\n",
    "\n",
    "        # Final linear projection\n",
    "        final_output = self.final_linear(decoder_output)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5c27c1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be used while training \n",
    "def create_padding_mask(seq, pad_idx):\n",
    "    return (seq != pad_idx).unsqueeze(1).unsqueeze(2)  # (batch, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    return torch.triu(torch.ones(size, size), diagonal=1).type(torch.bool).unsqueeze(0).unsqueeze(0)  # (1, 1, size, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c2490b",
   "metadata": {},
   "source": [
    "Testing the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c498eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Full Transformer Model...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "einsum(): subscript s has size 64 for operand 1 which does not broadcast with previously seen size 50",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[135], line 50\u001b[0m\n\u001b[0;32m     39\u001b[0m transformer_model \u001b[38;5;241m=\u001b[39m Transformer(\n\u001b[0;32m     40\u001b[0m     num_layers\u001b[38;5;241m=\u001b[39mnum_layers,\n\u001b[0;32m     41\u001b[0m     d_model\u001b[38;5;241m=\u001b[39md_model,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m     dropout_rate\u001b[38;5;241m=\u001b[39mdropout_rate\n\u001b[0;32m     47\u001b[0m )\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m output_logits \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdummy_src\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_tgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Print shapes to verify\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSource input shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdummy_src\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[133], line 65\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, tgt, src_mask, tgt_mask)\u001b[0m\n\u001b[0;32m     62\u001b[0m tgt_embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtgt_embedding(tgt))\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Encoder pass\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m encoder_output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_embedded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Decoder pass\u001b[39;00m\n\u001b[0;32m     68\u001b[0m decoder_output, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(tgt_embedded, encoder_output, src_mask, tgt_mask)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[130], line 19\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, x, src_mask)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, src_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 19\u001b[0m         x, _ \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#second return value is none (no attention weights are being returned)\\\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[129], line 31\u001b[0m, in \u001b[0;36mEncoderLayer.forward\u001b[1;34m(self, x, src_mask)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, src_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     30\u001b[0m     x_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrope(x) \u001b[38;5;66;03m#(batch_size, seq_len, d_model)\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_pos\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#self attention sublayer with prenorm includes residual connection internally\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn_norm(x) \u001b[38;5;66;03m#feed forward network sublayer with PreNorm\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[124], line 11\u001b[0m, in \u001b[0;36mPreNorm.forward\u001b[1;34m(self, x, key_input, value_input)\u001b[0m\n\u001b[0;32m      9\u001b[0m normalized_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m value_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 11\u001b[0m     sublayer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msublayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalized_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     sublayer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublayer(normalized_output, key_input\u001b[38;5;241m=\u001b[39mkey_input, value_input\u001b[38;5;241m=\u001b[39mvalue_input)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[127], line 84\u001b[0m, in \u001b[0;36mLinearAttention.forward\u001b[1;34m(self, query_input, key_input, value_input, mask)\u001b[0m\n\u001b[0;32m     81\u001b[0m K \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_k)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     82\u001b[0m V \u001b[38;5;241m=\u001b[39m V\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_k)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 84\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(batch_size , \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model)\n\u001b[0;32m     87\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_o(attn_output)\n",
      "Cell \u001b[1;32mIn[127], line 56\u001b[0m, in \u001b[0;36mLinearAttention.linear_attention\u001b[1;34m(self, Q, K, V)\u001b[0m\n\u001b[0;32m     53\u001b[0m K_feat_V \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbhsv, bhsd-> bhvd\u001b[39m\u001b[38;5;124m'\u001b[39m , K_feat, V)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m#denominator: (b, h, r)\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m z_denom \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbhsr , bhs->bhr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ_feat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK_feat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-6\u001b[39m)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m#numerator: (b, h, s, d)\u001b[39;00m\n\u001b[0;32m     59\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbhsr, bhvd->bhsd\u001b[39m\u001b[38;5;124m'\u001b[39m, Q_feat, K_feat_V)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\functional.py:407\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    409\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[1;31mRuntimeError\u001b[0m: einsum(): subscript s has size 64 for operand 1 which does not broadcast with previously seen size 50"
     ]
    }
   ],
   "source": [
    "print(\"Testing Full Transformer Model...\")\n",
    "\n",
    "num_layers = 2\n",
    "d_model = 256 \n",
    "num_heads = 4\n",
    "d_ff = 1024 \n",
    "input_vocab_size = 1000 \n",
    "target_vocab_size = 800 \n",
    "dropout_rate = 0.1\n",
    "PAD_IDX = 0 \n",
    "\n",
    "batch_size = 2 \n",
    "src_seq_len_test = 50\n",
    "tgt_seq_len_test = 40\n",
    "\n",
    "# Dummy input data\n",
    "dummy_src = torch.randint(1, input_vocab_size, (batch_size, src_seq_len_test))\n",
    "dummy_tgt = torch.randint(1, target_vocab_size, (batch_size, tgt_seq_len_test))\n",
    "\n",
    "# Apply padding\n",
    "dummy_src[0, 45:] = PAD_IDX\n",
    "dummy_tgt[1, 30:] = PAD_IDX\n",
    "\n",
    "# Create masks\n",
    "def create_padding_mask(seq, pad_idx):\n",
    "    return (seq != pad_idx).unsqueeze(1).unsqueeze(2)  # (batch, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    return torch.triu(torch.ones(size, size), diagonal=1).type(torch.bool).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "src_padding_mask = create_padding_mask(dummy_src, PAD_IDX)\n",
    "tgt_padding_mask = create_padding_mask(dummy_tgt, PAD_IDX)\n",
    "look_ahead_mask = create_look_ahead_mask(tgt_seq_len_test)\n",
    "\n",
    "# Combine padding and look-ahead masks\n",
    "tgt_mask = tgt_padding_mask & (create_look_ahead_mask(tgt_seq_len_test) == 0)\n",
    "\n",
    "# Initialize model\n",
    "transformer_model = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    d_ff=d_ff,\n",
    "    input_vocab_size=input_vocab_size,\n",
    "    target_vocab_size=target_vocab_size,\n",
    "    dropout_rate=dropout_rate\n",
    ")\n",
    "\n",
    "# Forward pass\n",
    "output_logits = transformer_model(dummy_src, dummy_tgt, src_padding_mask, tgt_mask)\n",
    "\n",
    "# Print shapes to verify\n",
    "print(f\"\\nSource input shape: {dummy_src.shape}\")\n",
    "print(f\"Target input shape: {dummy_tgt.shape}\")\n",
    "print(f\"Output logits shape: {output_logits.shape}\")\n",
    "\n",
    "# Assert output shape\n",
    "assert output_logits.shape == (batch_size, tgt_seq_len_test, target_vocab_size), \"Output shape mismatch!\"\n",
    "\n",
    "print(\"\\nâœ… Full Transformer Model test passed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e6b0a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
