{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcc63bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8a3cac",
   "metadata": {},
   "source": [
    "using PreNorm instead of AddNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7ac23d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, d_model, sublayer, dropout_rate=0.1):\n",
    "        super(PreNorm, self).__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.sublayer = sublayer\n",
    "\n",
    "    def forward(self,x):\n",
    "        normalized_output = self.norm(x)\n",
    "        sublayer_output = self.sublayer(normalized_output)\n",
    "        dropped_output = self.dropout(sublayer_output)\n",
    "        return x + dropped_output\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2275e544",
   "metadata": {},
   "source": [
    "using GLU instead of Position wise FFN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce6e3d7",
   "metadata": {},
   "source": [
    "using GELU instead of ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81ea56c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLUFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=None, dropout_rate = 0.1):\n",
    "        super(GLUFeedForward, self).__init__()\n",
    "        d_ff = d_ff or d_model\n",
    "        self.gate_proj = nn.Linear(d_model , d_ff)\n",
    "        self.value_proj = nn.Linear(d_model, d_ff)\n",
    "        self.output_proj = nn.Linear(d_ff, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate = self.gate_proj(x)\n",
    "        value = self.value_proj(x)\n",
    "\n",
    "        x = F.gelu(gate)*value\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        #project back to d_model\n",
    "        x = self.output_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9642da",
   "metadata": {},
   "source": [
    "updating Multi head attention with FAVOR+ Fast attention via positive orthogonal random features kindof linear attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6b1e271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def orthogonal_random_features(dim, num_heads, head_dim, device):\n",
    "    rand_proj = torch.randn(num_heads, dim, head_dim, device=device)\n",
    "    q, _ = torch.linalg.qr(rand_proj)\n",
    "    return q \n",
    "\n",
    "\n",
    "def elu_kernal(x):\n",
    "    return F.elu(x) + 1\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, d_model , num_heads, dropout_rate = 0.1, num_rfs = 10):\n",
    "        super(LinearAttention, self).__init__()\n",
    "        assert d_model % num_heads ==0\n",
    "\n",
    "        self.d_k =d_model // num_heads\n",
    "        self.d_model = d_model\n",
    "        self.num_rfs = num_rfs # number of random features\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        #Learnable scale Parameter for queries / keys\n",
    "        self.scale = nn.Parameter(torch.tensor(d_model ** -0.5))\n",
    "\n",
    "    def linear_attention(self, Q, K, V):\n",
    "        # Q, K, V : shape = (batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "        batch_size, num_heads, seq_len, head_dim = Q.shape\n",
    "\n",
    "        #random feature projection one per head\n",
    "        if not hasattr(self, 'rand_projs'):\n",
    "            self.rand_projs = orthogonal_random_features(\n",
    "                self.d_model, self.num_heads , self.num_rfs, Q.device\n",
    "            )\n",
    "\n",
    "        #projecting Q and K through random features\n",
    "        Q = Q.unsqueeze(-2) #(b, h, s, 1, d_k)\n",
    "        K = K.unsqueeze(-2)\n",
    "\n",
    "        Q_rand = torch.matmul(Q, self.rand_projs[:num_heads])  #(b, h, s, 1, r)\n",
    "        K_rand = torch.matmul(K, self.rand_projs[:num_heads]) #(b,h,s,1,r)\n",
    "\n",
    "        #applying kernal function\n",
    "        Q_feat = elu_kernal(Q_rand).squeeze(-2)    #(b, h, s, r)\n",
    "        K_feat = elu_kernal(K_rand).squeeze(-2)  #(b, h, s, r)\n",
    "\n",
    "        #compute KV numerator: (b,h,r,d)\n",
    "        K_feat_V = torch.einsum('bhsv, bhsd-> bhvd' , K_feat, V)\n",
    "\n",
    "        #denominator: (b, h, r)\n",
    "        z_denom = 1. / (torch.einsum('bhsr , bhs->bhr', Q_feat, torch.sum(K_feat, dim=2)) + 1e-6)\n",
    "\n",
    "        #numerator: (b, h, s, d)\n",
    "        attn_output = torch.einsum('bhsr, bhvd->bhsd', Q_feat, K_feat_V)\n",
    "\n",
    "        #normalize\n",
    "        attn_output = attn_output * z_denom.unsqueeze(-1)\n",
    "\n",
    "        return attn_output\n",
    "    \n",
    "\n",
    "    def forward(self, query_input, key_input , value_input, mask=None):\n",
    "        batch_size = query_input.size(0)\n",
    "\n",
    "        Q = self.W_q(query_input)\n",
    "        K = self.W_k(key_input)\n",
    "        V = self.W_v(value_input)\n",
    "\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
    "\n",
    "        attn_output = self.linear_attention(Q, K , V)\n",
    "\n",
    "        attn_output = attn_output.transpose(1,2).contiguous().view(batch_size , -1, self.d_model)\n",
    "        output = self.W_o(attn_output)\n",
    "\n",
    "        return output, None #no attention weights returned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000ab4ec",
   "metadata": {},
   "source": [
    "using RoPE instead of normal Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c94d5b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len=2048, base=10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.base = base\n",
    "\n",
    "        # Compute inverse frequency vector\n",
    "        inv_freq = 1. / (self.base ** (torch.arange(0, self.dim, 2).float() / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "\n",
    "        # Precompute position encodings\n",
    "        t = torch.arange(self.max_seq_len, dtype=torch.float32)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)  # Shape: (seq_len, dim)\n",
    "\n",
    "        # Apply sin/cos to create positional embeddings\n",
    "        self.register_buffer(\"cos_cached\", emb.cos()[None, :, :])\n",
    "        self.register_buffer(\"sin_cached\", emb.sin()[None, :, :])\n",
    "\n",
    "    def forward(self, x, seq_dim=1):\n",
    "        seq_len = x.shape[seq_dim]\n",
    "\n",
    "        if seq_dim == 0:\n",
    "            cos_pos = self.cos_cached[:, :seq_len, ...]\n",
    "            sin_pos = self.sin_cached[:, :seq_len, ...]\n",
    "        else:\n",
    "            cos_pos = self.cos_cached[:, :seq_len, ...]\n",
    "            sin_pos = self.sin_cached[:, :seq_len, ...]\n",
    "\n",
    "        x_rotated = rotate_half(x)\n",
    "        return (x * cos_pos) + (x_rotated * sin_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3258f8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model , num_heads, d_ff, dropout_rate=0.1, num_rfs=64):\n",
    "        super(EncoderLayer,self).__init__()\n",
    "\n",
    "        self.self_attn = LinearAttention(\n",
    "            d_model = d_model,\n",
    "            num_heads= num_heads,\n",
    "            dropout_rate= dropout_rate,\n",
    "            num_rfs=num_rfs\n",
    "        )\n",
    "\n",
    "        #preNorm wrappers \n",
    "        self.attn_norm = PreNorm(d_model , self.self_attn)\n",
    "\n",
    "        # GLU FFN\n",
    "        self.ffn = GLUFeedForward(\n",
    "            d_model=d_model,\n",
    "            d_ff=d_ff,\n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "\n",
    "        self.ffn_norm = PreNorm(d_model, self.ffn)\n",
    "\n",
    "\n",
    "        # RoPE\n",
    "        self.rope = RotaryPositionalEmbedding(d_model)\n",
    "\n",
    "\n",
    "    def forward(self, x, src_mask = None):\n",
    "        x_pos = self.rope(x) #(batch_size, seq_len, d_model)\n",
    "        x = self.attn_norm(x_pos) #self attention sublayer with prenorm includes residual connection internally\n",
    "        x = self.ffn_norm(x) #feed forward network sublayer with PreNorm\n",
    "\n",
    "\n",
    "        return x, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f1c3212",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, dropout_rate=0.1, num_rfs=64):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(\n",
    "                d_model = d_model,\n",
    "                num_heads= num_heads,\n",
    "                d_ff = d_ff,\n",
    "                dropout_rate=dropout_rate,\n",
    "                num_rfs= num_rfs\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        def forward(self, x, src_mask = None):\n",
    "            for layer in self.layers:\n",
    "                x, _ = layer(x, src_mask) #second return value is none (no attention weights are being returned)\\\n",
    "            return x, None\n",
    "        \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c702cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model , d_ff , num_heads, dropout_rate= 0.1, num_rfs = 64):\n",
    "        super(DecoderLayer, self)\n",
    "        \n",
    "        self.masked_self_attn = LinearAttention(\n",
    "            d_model = d_model,\n",
    "            num_heads= num_heads,\n",
    "            dropout_rate= dropout_rate,\n",
    "            num_rfs=num_rfs\n",
    "        )\n",
    "\n",
    "        self.self_attn_norm= PreNorm(d_model, self.masked_self_attn)\n",
    "\n",
    "#encoder decoder cross attention\n",
    "        self.encoder_decoder_attn = LinearAttention(\n",
    "            d_model= d_model,\n",
    "            num_heads= num_heads,\n",
    "            dropout_rate= dropout_rate,\n",
    "            num_rfs= num_rfs\n",
    "        )\n",
    "\n",
    "        self.encoder_decoder_attn = PreNorm(d_model, self.encoder_decoder_attn)\n",
    "\n",
    "            # GLU FFN\n",
    "        self.ffn = GLUFeedForward(\n",
    "            d_model=d_model,\n",
    "            d_ff=d_ff,\n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "\n",
    "        self.ffn_norm = PreNorm(d_model, self.ffn)\n",
    "\n",
    "        self.rope = RotaryPositionalEmbedding(d_model)\n",
    "\n",
    "\n",
    "    def forward(self, x , encoder_output , src_mask = None, tgt_mask= None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input to decoder (batch_size, tgt_seq_len, d_model)\n",
    "            encoder_output: Output from encoder (batch_size, src_seq_len, d_model)\n",
    "            src_mask: Mask for encoder outputs (optional)\n",
    "            tgt_mask: Causal mask for decoder self-attention (optional)\n",
    "\n",
    "        Returns:\n",
    "            x: Updated decoder output\n",
    "            None, None: Placeholder for attention weights (not returned by linear attention)\n",
    "        \"\"\"\n",
    "        \n",
    "        x_pos = self.rope(x)\n",
    "        x = self.self_attn_norm(x_pos)\n",
    "        x = self.encoder_decoder_attn(x)\n",
    "        x = self.ffn_norm(x)\n",
    "\n",
    "        return x, None , None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54943f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model , num_heads, d_ff, dropout_rate = 0.1 , num_rfs = 64):\n",
    "        super(Decoder , self)\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(\n",
    "                d_model = d_model,\n",
    "                num_heads= num_heads,\n",
    "                d_ff = d_ff,\n",
    "                dropout_rate=dropout_rate,\n",
    "                num_rfs= num_rfs\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x , encoder_output , src_mask = None, tgt_mask = None):\n",
    "        for layer in self.layers:\n",
    "            x, _, _ = layer(x , encoder_output, src_mask , tgt_mask)\n",
    "        return x, None, None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a77e8638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers,\n",
    "        d_model,\n",
    "        num_heads,\n",
    "        d_ff,\n",
    "        input_vocab_size,\n",
    "        target_vocab_size,\n",
    "        dropout_rate=0.1,\n",
    "        num_rfs=64  # Number of random features for Linear Attention\n",
    "    ):\n",
    "        super(Transformer , self).__init__()\n",
    "\n",
    "        # Input Embeddings\n",
    "        self.src_embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "\n",
    "        # Encoder and Decoder Stacks (with RoPE, Linear Attention, GLU, PreNorm)\n",
    "        self.encoder = Encoder(\n",
    "            num_layers=num_layers,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            d_ff=d_ff,\n",
    "            dropout_rate=dropout_rate,\n",
    "            num_rfs=num_rfs\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            num_layers=num_layers,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            d_ff=d_ff,\n",
    "            dropout_rate=dropout_rate,\n",
    "            num_rfs=num_rfs\n",
    "        )\n",
    "\n",
    "        # Final projection layer\n",
    "        self.final_linear = nn.Linear(d_model, target_vocab_size)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Source token indices (batch_size, src_seq_len)\n",
    "            tgt: Target token indices (batch_size, tgt_seq_len)\n",
    "            src_mask: Optional mask for encoder (batch_size, 1, 1, src_seq_len)\n",
    "            tgt_mask: Optional causal mask for decoder (1, 1, tgt_seq_len, tgt_seq_len)\n",
    "\n",
    "        Returns:\n",
    "            final_output: Logits over target vocabulary\n",
    "            None: Placeholder (no attention weights returned in linear attention)\n",
    "        \"\"\"\n",
    "\n",
    "        # Input embeddings\n",
    "        src_embedded = self.dropout(self.src_embedding(src))\n",
    "        tgt_embedded = self.dropout(self.tgt_embedding(tgt))\n",
    "\n",
    "        # Encoder pass\n",
    "        encoder_output, _ = self.encoder(src_embedded, src_mask)\n",
    "\n",
    "        # Decoder pass\n",
    "        decoder_output, _, _ = self.decoder(tgt_embedded, encoder_output, src_mask, tgt_mask)\n",
    "\n",
    "        # Final linear projection\n",
    "        final_output = self.final_linear(decoder_output)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c27c1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be used while training \n",
    "def create_padding_mask(seq, pad_idx):\n",
    "    return (seq != pad_idx).unsqueeze(1).unsqueeze(2)  # (batch, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    return torch.triu(torch.ones(size, size), diagonal=1).type(torch.bool).unsqueeze(0).unsqueeze(0)  # (1, 1, size, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c2490b",
   "metadata": {},
   "source": [
    "Testing the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c498eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing Full Transformer Model...\")\n",
    "\n",
    "num_layers = 2\n",
    "d_model = 256 \n",
    "num_heads = 4\n",
    "d_ff = 1024 \n",
    "input_vocab_size = 1000 \n",
    "target_vocab_size = 800 \n",
    "dropout_rate = 0.1\n",
    "PAD_IDX = 0 \n",
    "\n",
    "batch_size = 2 \n",
    "src_seq_len_test = 50\n",
    "tgt_seq_len_test = 40\n",
    "\n",
    "# Dummy input data\n",
    "dummy_src = torch.randint(1, input_vocab_size, (batch_size, src_seq_len_test))\n",
    "dummy_tgt = torch.randint(1, target_vocab_size, (batch_size, tgt_seq_len_test))\n",
    "\n",
    "# Apply padding\n",
    "dummy_src[0, 45:] = PAD_IDX\n",
    "dummy_tgt[1, 30:] = PAD_IDX\n",
    "\n",
    "# Create masks\n",
    "def create_padding_mask(seq, pad_idx):\n",
    "    return (seq != pad_idx).unsqueeze(1).unsqueeze(2)  # (batch, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    return torch.triu(torch.ones(size, size), diagonal=1).type(torch.bool).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "src_padding_mask = create_padding_mask(dummy_src, PAD_IDX)\n",
    "tgt_padding_mask = create_padding_mask(dummy_tgt, PAD_IDX)\n",
    "look_ahead_mask = create_look_ahead_mask(tgt_seq_len_test)\n",
    "\n",
    "# Combine padding and look-ahead masks\n",
    "tgt_mask = tgt_padding_mask & (create_look_ahead_mask(tgt_seq_len_test) == 0)\n",
    "\n",
    "# Initialize model\n",
    "transformer_model = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    d_ff=d_ff,\n",
    "    input_vocab_size=input_vocab_size,\n",
    "    target_vocab_size=target_vocab_size,\n",
    "    dropout_rate=dropout_rate\n",
    ")\n",
    "\n",
    "# Forward pass\n",
    "output_logits = transformer_model(dummy_src, dummy_tgt, src_padding_mask, tgt_mask)\n",
    "\n",
    "# Print shapes to verify\n",
    "print(f\"\\nSource input shape: {dummy_src.shape}\")\n",
    "print(f\"Target input shape: {dummy_tgt.shape}\")\n",
    "print(f\"Output logits shape: {output_logits.shape}\")\n",
    "\n",
    "# Assert output shape\n",
    "assert output_logits.shape == (batch_size, tgt_seq_len_test, target_vocab_size), \"Output shape mismatch!\"\n",
    "\n",
    "print(\"\\nâœ… Full Transformer Model test passed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e6b0a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
