{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5309489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc15fe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PreNorm instead of AddNorm\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, d_model, sublayer_fn, dropout_rate=0.1): # Renamed sublayer to sublayer_fn\n",
    "        super(PreNorm, self).__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.sublayer_fn = sublayer_fn # Renamed\n",
    "\n",
    "    def forward(self, x, sublayer_input=None, key_input=None, value_input=None, mask=None):\n",
    "        normalized_x = self.norm(x)\n",
    "\n",
    "        # Handle different sublayer types\n",
    "        if \"GLUFeedForward\" in str(self.sublayer_fn.__class__):\n",
    "            sublayer_output = self.sublayer_fn(normalized_x)\n",
    "        elif \"LinearAttention\" in str(self.sublayer_fn.__class__):\n",
    "            if key_input is not None and value_input is not None: # Cross-attention\n",
    "                sublayer_output, _ = self.sublayer_fn(normalized_x, key_input=key_input, value_input=value_input, mask=mask)\n",
    "            else: \n",
    "                # normalized_x is used as Q, K, V\n",
    "                sublayer_output, _ = self.sublayer_fn(normalized_x, key_input=normalized_x, value_input=normalized_x, mask=mask)\n",
    "        else: \n",
    "            if key_input is None and value_input is None:\n",
    "                 sublayer_output = self.sublayer_fn(normalized_x)\n",
    "            else:\n",
    "                 sublayer_output = self.sublayer_fn(normalized_x, key_input=key_input, value_input=value_input)\n",
    "\n",
    "\n",
    "        dropped_output = self.dropout(sublayer_output)\n",
    "        return x + dropped_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "223ebf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using GLU instead of Position wise FFN\n",
    "# using GELU instead of ReLU\n",
    "class GLUFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=None, dropout_rate = 0.1):\n",
    "        super(GLUFeedForward, self).__init__()\n",
    "        hidden_dim = d_ff or 4 * d_model \n",
    "        self.gate_proj = nn.Linear(d_model , hidden_dim)\n",
    "        self.value_proj = nn.Linear(d_model, hidden_dim)\n",
    "        self.output_proj = nn.Linear(hidden_dim, d_model)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate = self.gate_proj(x)\n",
    "        value = self.value_proj(x)\n",
    "        x = F.gelu(gate) * value # GELU for the gate\n",
    "        x = self.dropout(x)\n",
    "        x = self.output_proj(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3c00291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating Multi head attention with FAVOR+ Fast attention\n",
    "# via positive orthogonal random features kindof linear attention\n",
    "\n",
    "def orthogonal_random_features(dim, num_heads, num_rfs, device):\n",
    "\n",
    "    # rand_proj shape: (num_heads, num_rfs, dim)\n",
    "    rand_proj = torch.randn(num_heads, num_rfs, dim, device=device)\n",
    "    q, _ = torch.linalg.qr(rand_proj.transpose(-2, -1))\n",
    "    return q.transpose(-2, -1)\n",
    "\n",
    "\n",
    "def elu_kernel(x): # Changed from kernal to kernel\n",
    "    return F.elu(x) + 1.0 # Add 1.0 to ensure positivity for FAVOR+\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, d_model , num_heads, dropout_rate = 0.1, num_rfs = 64): # num_rfs default from encoder\n",
    "        super(LinearAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.d_model = d_model\n",
    "        self.num_rfs = num_rfs # number of random features\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "\n",
    "        self.rand_projs = None # Initialize placeholder\n",
    "\n",
    "    def _init_random_features(self, head_dim, device):\n",
    "       \n",
    "        if self.rand_projs is None or self.rand_projs.shape[-1] != head_dim or self.rand_projs.device != device :\n",
    "             self.rand_projs = orthogonal_random_features(\n",
    "                head_dim, self.num_heads, self.num_rfs, device\n",
    "            )\n",
    "\n",
    "\n",
    "    def _apply_mask(self, tensor, mask):\n",
    "        if mask is None:\n",
    "            return tensor\n",
    "        \n",
    "       \n",
    "        if mask.ndim == 4 and mask.shape[1] == 1 and mask.shape[2] == 1: # (b, 1, 1, s_kv)\n",
    "            mask = mask.transpose(-2, -1) # (b, 1, s_kv, 1)\n",
    "        elif mask.ndim == 2: # (b, s_kv)\n",
    "            mask = mask.unsqueeze(1).unsqueeze(-1) # (b, 1, s_kv, 1)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        if tensor.dtype == torch.bool:\n",
    "             return tensor.masked_fill(mask == 0, False) # Assuming mask 0 is pad\n",
    "        else:\n",
    "             return tensor.masked_fill(mask == 0, 0.0)\n",
    "\n",
    "\n",
    "    def linear_attention(self, Q, K, V, key_padding_mask=None):\n",
    "        # Q, K, V : shape = (batch_size, num_heads, seq_len, head_dim)\n",
    "        # key_padding_mask: (batch_size, 1, 1, seq_len_kv) or (batch_size, seq_len_kv)\n",
    "        batch_size, num_heads, seq_len_q, head_dim = Q.shape\n",
    "        _, _, seq_len_kv, _ = K.shape\n",
    "\n",
    "\n",
    "        self._init_random_features(head_dim, Q.device)\n",
    "\n",
    "        # Projecting Q and K through random features\n",
    "        # Q: (b, h, s_q, d), self.rand_projs: (h, r, d) -> Q_rand: (b, h, s_q, r)\n",
    "        Q_rand = torch.einsum('bhsd,hrd->bhsr', Q, self.rand_projs)\n",
    "        K_rand = torch.einsum('bhsd,hrd->bhsr', K, self.rand_projs)\n",
    "\n",
    "        # Applying kernel function (elu + 1 to ensure positivity)\n",
    "        Q_feat = elu_kernel(Q_rand)    # (b, h, s_q, r)\n",
    "        K_feat = elu_kernel(K_rand)    # (b, h, s_kv, r)\n",
    "\n",
    "        # Apply padding mask to K_feat and V BEFORE computations\n",
    "        if key_padding_mask is not None:\n",
    "            # Transpose mask from (B, 1, 1, S_kv) to (B, 1, S_kv, 1) for broadcasting\n",
    "            _mask = key_padding_mask.transpose(-2, -1).to(K_feat.device) # (B, 1, S_kv, 1)\n",
    "            K_feat = K_feat.masked_fill(_mask == 0, 0.0)\n",
    "            V = V.masked_fill(_mask.expand_as(V) == 0, 0.0)\n",
    "\n",
    "\n",
    "        K_feat_V = torch.einsum('bhsr,bhsd->bhrd', K_feat, V)\n",
    "\n",
    "       \n",
    "        numerator = torch.einsum('bhsr,bhrd->bhsd', Q_feat, K_feat_V)\n",
    "\n",
    "      \n",
    "        K_feat_sum_across_seq = torch.sum(K_feat, dim=2)  # (b, h, r)\n",
    "        denominator = torch.einsum('bhsr,bhr->bhs', Q_feat, K_feat_sum_across_seq) # (b, h, s_q)\n",
    "        attn_output = numerator / (denominator.unsqueeze(-1) + 1e-6)\n",
    "\n",
    "\n",
    "        return attn_output\n",
    "\n",
    "\n",
    "    def forward(self, query_input, key_input=None, value_input=None, mask=None):\n",
    "       \n",
    "        batch_size = query_input.size(0)\n",
    "\n",
    "        \n",
    "        if key_input is None:\n",
    "            key_input = query_input\n",
    "        if value_input is None:\n",
    "            value_input = query_input \n",
    "\n",
    "        # Linear projections\n",
    "        Q = self.W_q(query_input)\n",
    "        K = self.W_k(key_input)\n",
    "        V = self.W_v(value_input)\n",
    "\n",
    "        # Reshape and transpose for multi-head attention\n",
    "        # (batch_size, seq_len, d_model) -> (batch_size, num_heads, seq_len, d_k)\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
    "\n",
    "        # Apply linear attention\n",
    "\n",
    "        attn_output = self.linear_attention(Q, K, V, key_padding_mask=mask)\n",
    "\n",
    "        # Concatenate heads and apply final linear layer\n",
    "        attn_output = attn_output.transpose(1,2).contiguous().view(batch_size , -1, self.d_model)\n",
    "        output = self.W_o(attn_output)\n",
    "        output = self.dropout(output) # Apply dropout on the final output of MHA\n",
    "\n",
    "        return output, None # No attention weights returned for linear attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2489781c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using RoPE instead of normal Positional Encoding\n",
    "def rotate_half(x):\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len=2048, base=10000):\n",
    "        super().__init__()\n",
    "        assert dim % 2 == 0, \"Dimension must be even for RoPE.\"\n",
    "        self.dim = dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.base = base\n",
    "\n",
    "       \n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False) \n",
    "\n",
    "       \n",
    "        t = torch.arange(self.max_seq_len).type_as(self.inv_freq)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos().unsqueeze(0), persistent=False) # (1, max_seq_len, dim)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin().unsqueeze(0), persistent=False) # (1, max_seq_len, dim)\n",
    "\n",
    "\n",
    "    def forward(self, x, seq_dim=1):\n",
    "      \n",
    "        seq_len = x.shape[seq_dim]\n",
    "\n",
    "        if seq_len > self.max_seq_len:\n",
    "\n",
    "            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n",
    "            freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "            emb = torch.cat((freqs, freqs), dim=-1)\n",
    "            cos = emb.cos().unsqueeze(0)\n",
    "            sin = emb.sin().unsqueeze(0)\n",
    "        else:\n",
    "            cos = self.cos_cached[:, :seq_len, ...]\n",
    "            sin = self.sin_cached[:, :seq_len, ...]\n",
    "\n",
    "        \n",
    "        if x.ndim == 3: # (batch, seq_len, dim)\n",
    "             cos = cos.squeeze(0) # (seq_len, dim)\n",
    "             sin = sin.squeeze(0) # (seq_len, dim)\n",
    "       \n",
    "        return (x * cos) + (rotate_half(x) * sin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b37c4132",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model , num_heads, d_ff, dropout_rate=0.1, num_rfs=64):\n",
    "        super(EncoderLayer,self).__init__()\n",
    "\n",
    "        self.self_attn_module = LinearAttention( # Store module itself\n",
    "            d_model = d_model,\n",
    "            num_heads= num_heads,\n",
    "            dropout_rate= dropout_rate,\n",
    "            num_rfs=num_rfs\n",
    "        )\n",
    "        self.attn_norm = PreNorm(d_model , self.self_attn_module, dropout_rate=dropout_rate)\n",
    "\n",
    "        self.ffn_module = GLUFeedForward( # Store module itself\n",
    "            d_model=d_model,\n",
    "            d_ff=d_ff,\n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "        self.ffn_norm = PreNorm(d_model, self.ffn_module, dropout_rate=dropout_rate)\n",
    "        self.rope = RotaryPositionalEmbedding(d_model) # RoPE for d_model\n",
    "\n",
    "    def forward(self, x, src_mask = None): \n",
    "        x_with_rope = self.rope(x)\n",
    "\n",
    "        \n",
    "        x = self.attn_norm(x, sublayer_input=x_with_rope, mask=src_mask)\n",
    "\n",
    "        # FFN with PreNorm\n",
    "        x = self.ffn_norm(x)\n",
    "        return x, None # No attention weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc967e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, dropout_rate=0.1, num_rfs=64):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(\n",
    "                d_model = d_model,\n",
    "                num_heads= num_heads,\n",
    "                d_ff = d_ff,\n",
    "                dropout_rate=dropout_rate,\n",
    "                num_rfs= num_rfs\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "   \n",
    "\n",
    "    def forward(self, x, src_mask = None): # x is embedded_src\n",
    "       \n",
    "        for layer in self.layers:\n",
    "            x, _ = layer(x, src_mask)\n",
    "        return x, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fff4b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model , d_ff , num_heads, dropout_rate= 0.1, num_rfs = 64):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.masked_self_attn_module = LinearAttention(\n",
    "            d_model = d_model, num_heads= num_heads, dropout_rate= dropout_rate, num_rfs=num_rfs\n",
    "        )\n",
    "        self.self_attn_norm = PreNorm(d_model, self.masked_self_attn_module, dropout_rate=dropout_rate)\n",
    "\n",
    "        self.encoder_decoder_attn_module = LinearAttention(\n",
    "            d_model= d_model, num_heads= num_heads, dropout_rate= dropout_rate, num_rfs= num_rfs\n",
    "        )\n",
    "        # For cross-attention, query comes from decoder, K/V from encoder_output\n",
    "        self.encoder_decoder_attn_norm = PreNorm(d_model, self.encoder_decoder_attn_module, dropout_rate=dropout_rate)\n",
    "\n",
    "        self.ffn_module = GLUFeedForward(\n",
    "            d_model=d_model, d_ff=d_ff, dropout_rate=dropout_rate\n",
    "        )\n",
    "        self.ffn_norm = PreNorm(d_model, self.ffn_module, dropout_rate=dropout_rate)\n",
    "        self.rope = RotaryPositionalEmbedding(d_model) # RoPE for d_model\n",
    "\n",
    "    def forward(self, x , encoder_output , src_mask = None, tgt_mask= None):\n",
    "       \n",
    "      \n",
    "        x_with_rope = self.rope(x)\n",
    "\n",
    " \n",
    "        x = self.self_attn_norm(x, sublayer_input=x_with_rope, mask=tgt_mask)\n",
    "\n",
    "\n",
    "      \n",
    "        x = self.encoder_decoder_attn_norm(x, key_input=encoder_output, value_input=encoder_output, mask=src_mask)\n",
    "\n",
    "        # FFN\n",
    "        x = self.ffn_norm(x)\n",
    "        return x, None , None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69593dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model , num_heads, d_ff, dropout_rate = 0.1 , num_rfs = 64):\n",
    "        super(Decoder , self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(\n",
    "                d_model = d_model, d_ff = d_ff, num_heads= num_heads,\n",
    "                dropout_rate=dropout_rate, num_rfs= num_rfs\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "\n",
    "    def forward(self, x , encoder_output , src_mask = None, tgt_mask = None): # x is embedded_tgt\n",
    "        for layer in self.layers:\n",
    "            x, _, _ = layer(x , encoder_output, src_mask , tgt_mask)\n",
    "        return x, None, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d13ec13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers,\n",
    "        d_model,\n",
    "        num_heads,\n",
    "        d_ff,\n",
    "        input_vocab_size,\n",
    "        target_vocab_size,\n",
    "        dropout_rate=0.1,\n",
    "        num_rfs=64  # Number of random features for Linear Attention\n",
    "    ):\n",
    "        super(Transformer , self).__init__()\n",
    "\n",
    "        self.d_model = d_model # Store d_model\n",
    "        self.src_embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "\n",
    "       \n",
    "        self.encoder = Encoder(\n",
    "            num_layers=num_layers, d_model=d_model, num_heads=num_heads,\n",
    "            d_ff=d_ff, dropout_rate=dropout_rate, num_rfs=num_rfs\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            num_layers=num_layers, d_model=d_model, num_heads=num_heads,\n",
    "            d_ff=d_ff, dropout_rate=dropout_rate, num_rfs=num_rfs\n",
    "        )\n",
    "        self.final_linear = nn.Linear(d_model, target_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate) # General dropout for embeddings\n",
    "\n",
    "        # Initialize parameters (optional but good practice)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "    \n",
    "\n",
    "     \n",
    "        src_embedded = self.dropout(self.src_embedding(src) * math.sqrt(self.d_model))\n",
    "        tgt_embedded = self.dropout(self.tgt_embedding(tgt) * math.sqrt(self.d_model))\n",
    "\n",
    "       \n",
    "        encoder_output, _ = self.encoder(src_embedded, src_mask)\n",
    "\n",
    "     \n",
    "        decoder_output, _, _ = self.decoder(tgt_embedded, encoder_output, src_mask, tgt_mask)\n",
    "\n",
    "        final_output = self.final_linear(decoder_output)\n",
    "        return final_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eeeb3a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask creation functions\n",
    "def create_padding_mask(seq, pad_idx):\n",
    "   \n",
    "    return (seq != pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "def create_look_ahead_mask(size, device):\n",
    "   \n",
    "    mask = torch.triu(torch.ones(size, size, device=device), diagonal=1).type(torch.bool)\n",
    "    return (~mask).unsqueeze(0).unsqueeze(0) # Invert: True for non-masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66f8f058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Full Transformer Model...\n",
      "Using device: cuda\n",
      "\n",
      "Source input shape: torch.Size([2, 50])\n",
      "Target input shape: torch.Size([2, 40])\n",
      "Output logits shape: torch.Size([2, 40, 800])\n",
      "\n",
      "✅ Full Transformer Model test passed successfully (structurally)!\n",
      "Note: Logic of LinearAttention causality and full mask interaction needs careful consideration for semantic correctness beyond shape matching.\n"
     ]
    }
   ],
   "source": [
    "# --- Test Script ---\n",
    "print(\"Testing Full Transformer Model...\")\n",
    "\n",
    "# Hyperparameters\n",
    "num_layers = 2\n",
    "d_model = 256\n",
    "num_heads = 4\n",
    "d_ff = d_model * 4 # Standard FFN hidden dim\n",
    "input_vocab_size = 1000\n",
    "target_vocab_size = 800\n",
    "dropout_rate = 0.1\n",
    "PAD_IDX = 0\n",
    "num_rfs_test = 64 # For LinearAttention\n",
    "\n",
    "# Data parameters\n",
    "batch_size = 2\n",
    "src_seq_len_test = 50\n",
    "tgt_seq_len_test = 40\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Dummy input data\n",
    "dummy_src = torch.randint(1, input_vocab_size, (batch_size, src_seq_len_test), device=device)\n",
    "dummy_tgt = torch.randint(1, target_vocab_size, (batch_size, tgt_seq_len_test), device=device)\n",
    "\n",
    "# Apply padding\n",
    "dummy_src[0, src_seq_len_test-5:] = PAD_IDX\n",
    "dummy_tgt[1, tgt_seq_len_test-10:] = PAD_IDX\n",
    "\n",
    "# Create masks\n",
    "# src_padding_mask: True for non-pad, False for pad. Shape (B, 1, 1, S_src)\n",
    "src_padding_mask = create_padding_mask(dummy_src, PAD_IDX)\n",
    "\n",
    "# tgt_padding_mask: True for non-pad, False for pad. Shape (B, 1, 1, S_tgt)\n",
    "tgt_self_padding_mask = create_padding_mask(dummy_tgt, PAD_IDX)\n",
    "\n",
    "# look_ahead_mask: True for allowed, False for future. Shape (1, 1, S_tgt, S_tgt)\n",
    "look_ahead_causal_mask = create_look_ahead_mask(tgt_seq_len_test, device)\n",
    "\n",
    "\n",
    "decoder_self_attn_mask = tgt_self_padding_mask # (B,1,1,S_tgt)\n",
    "\n",
    "# For encoder-decoder attention, the mask is src_padding_mask (for encoder keys)\n",
    "decoder_cross_attn_mask = src_padding_mask\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "transformer_model = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    d_ff=d_ff,\n",
    "    input_vocab_size=input_vocab_size,\n",
    "    target_vocab_size=target_vocab_size,\n",
    "    dropout_rate=dropout_rate,\n",
    "    num_rfs=num_rfs_test\n",
    ").to(device)\n",
    "\n",
    "# Forward pass\n",
    "# Transformer.forward expects: src, tgt, src_mask (for encoder and cross-attn), tgt_mask (for decoder self-attn)\n",
    "output_logits = transformer_model(dummy_src, dummy_tgt, decoder_cross_attn_mask, decoder_self_attn_mask)\n",
    "\n",
    "# Print shapes to verify\n",
    "print(f\"\\nSource input shape: {dummy_src.shape}\")\n",
    "print(f\"Target input shape: {dummy_tgt.shape}\")\n",
    "print(f\"Output logits shape: {output_logits.shape}\")\n",
    "\n",
    "# Assert output shape\n",
    "assert output_logits.shape == (batch_size, tgt_seq_len_test, target_vocab_size), \"Output shape mismatch!\"\n",
    "\n",
    "print(\"\\n✅ Full Transformer Model test passed successfully (structurally)!\")\n",
    "print(\"Note: Logic of LinearAttention causality and full mask interaction needs careful consideration for semantic correctness beyond shape matching.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
